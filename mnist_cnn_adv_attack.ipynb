{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4a44c702f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Backprop and perform Adam optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "\n",
    "print(\"Starting experiment...\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Normal Training\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 6\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = 'datasets/'\n",
    "MODEL_STORE_PATH = 'trained_models/'\n",
    "\n",
    "# transforms to apply to the data\n",
    "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Epoch [1/6], Step [100/600], Loss: 2.3018, Accuracy: 11.00%\n",
      "Epoch [1/6], Step [200/600], Loss: 2.2533, Accuracy: 21.00%\n",
      "Epoch [1/6], Step [300/600], Loss: 2.1724, Accuracy: 23.00%\n",
      "Epoch [1/6], Step [400/600], Loss: 2.1385, Accuracy: 28.00%\n",
      "Epoch [1/6], Step [500/600], Loss: 2.0158, Accuracy: 29.00%\n",
      "Epoch [1/6], Step [600/600], Loss: 1.8882, Accuracy: 29.00%\n",
      "Epoch [2/6], Step [100/600], Loss: 1.9639, Accuracy: 33.00%\n",
      "Epoch [2/6], Step [200/600], Loss: 1.7675, Accuracy: 33.00%\n",
      "Epoch [2/6], Step [300/600], Loss: 1.7053, Accuracy: 37.00%\n",
      "Epoch [2/6], Step [400/600], Loss: 1.8603, Accuracy: 36.00%\n",
      "Epoch [2/6], Step [500/600], Loss: 1.7498, Accuracy: 36.00%\n",
      "Epoch [2/6], Step [600/600], Loss: 1.6862, Accuracy: 44.00%\n",
      "Epoch [3/6], Step [100/600], Loss: 1.5301, Accuracy: 43.00%\n",
      "Epoch [3/6], Step [200/600], Loss: 1.5630, Accuracy: 42.00%\n",
      "Epoch [3/6], Step [300/600], Loss: 1.4773, Accuracy: 48.00%\n",
      "Epoch [3/6], Step [400/600], Loss: 1.6341, Accuracy: 37.00%\n",
      "Epoch [3/6], Step [500/600], Loss: 1.6552, Accuracy: 42.00%\n",
      "Epoch [3/6], Step [600/600], Loss: 1.5392, Accuracy: 47.00%\n",
      "Epoch [4/6], Step [100/600], Loss: 1.4885, Accuracy: 49.00%\n",
      "Epoch [4/6], Step [200/600], Loss: 1.5012, Accuracy: 43.00%\n",
      "Epoch [4/6], Step [300/600], Loss: 1.5646, Accuracy: 45.00%\n",
      "Epoch [4/6], Step [400/600], Loss: 1.4363, Accuracy: 52.00%\n",
      "Epoch [4/6], Step [500/600], Loss: 1.2868, Accuracy: 50.00%\n",
      "Epoch [4/6], Step [600/600], Loss: 1.3752, Accuracy: 54.00%\n",
      "Epoch [5/6], Step [100/600], Loss: 1.2823, Accuracy: 51.00%\n",
      "Epoch [5/6], Step [200/600], Loss: 1.1332, Accuracy: 56.00%\n",
      "Epoch [5/6], Step [300/600], Loss: 1.3148, Accuracy: 54.00%\n",
      "Epoch [5/6], Step [400/600], Loss: 1.2443, Accuracy: 62.00%\n",
      "Epoch [5/6], Step [500/600], Loss: 1.1802, Accuracy: 60.00%\n",
      "Epoch [5/6], Step [600/600], Loss: 1.3242, Accuracy: 51.00%\n",
      "Epoch [6/6], Step [100/600], Loss: 1.2661, Accuracy: 59.00%\n",
      "Epoch [6/6], Step [200/600], Loss: 1.2854, Accuracy: 54.00%\n",
      "Epoch [6/6], Step [300/600], Loss: 1.0983, Accuracy: 62.00%\n",
      "Epoch [6/6], Step [400/600], Loss: 0.9857, Accuracy: 63.00%\n",
      "Epoch [6/6], Step [500/600], Loss: 1.2300, Accuracy: 59.00%\n",
      "Epoch [6/6], Step [600/600], Loss: 1.0453, Accuracy: 67.00%\n",
      "Test Accuracy of the model on the 10000 test images: 95.24000000000001 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "\n",
    "print(\"Starting experiment...\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Adversarial Training\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 6\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "DATA_PATH = 'datasets/'\n",
    "MODEL_STORE_PATH = 'trained_models/'\n",
    "\n",
    "# transforms to apply to the data\n",
    "#trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 2000)\n",
    "        self.fc2 = nn.Linear(2000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "rob_thold = 0.3\n",
    "lrate_adv = 4e-2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images_original = images.clone()\n",
    "\n",
    "        for j in range(10):\n",
    "            images.requires_grad = True\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                images_grad_step = lrate_adv*torch.sign(images.grad)\n",
    "\n",
    "                #Take grad step\n",
    "                images += images_grad_step\n",
    "\n",
    "                #Project\n",
    "                images = images_original + torch.clamp(images-images_original, -rob_thold, rob_thold)\n",
    "                images = torch.clamp(images,0,1)\n",
    "        \n",
    "        images.requires_grad = False\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 83.35000000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model on adversrial examples\n",
    "from torch.autograd import Variable\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "rob_thold = 0.2\n",
    "lrate_adv = 4e-2\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    images_original = images.clone()\n",
    "    \n",
    "    for i in range(50):\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            ##l-inf attack\n",
    "            images_grad_step = lrate_adv*torch.sign(images.grad)\n",
    "            \n",
    "            #Take grad step\n",
    "            images += images_grad_step\n",
    "            \n",
    "            #Project\n",
    "            images = images_original + torch.clamp(images-images_original, -rob_thold, rob_thold)\n",
    "            images = torch.clamp(images,0,1)\n",
    "            ##l-inf attack ends\n",
    "        \n",
    "\n",
    "            \n",
    "    outputs = model(images)  \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6803eaeeb8>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6804066fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "images_np = images.cpu().numpy()\n",
    "plt.imshow(images_np[10,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 524,
   "position": {
    "height": "499px",
    "left": "1225.97px",
    "right": "20px",
    "top": "46.9688px",
    "width": "252px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
