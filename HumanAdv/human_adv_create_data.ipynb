{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "[0, 1, 5]\n",
      "(18000, 784) (18000,) (3000, 784) (3000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "import os\n",
    "from scipy import stats\n",
    "import struct\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.linalg import eigh\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy as sp\n",
    "import math\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "import png\n",
    "import imageio\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "def crop_center(img,cropy, cropx):\n",
    "    y,x = img.shape\n",
    "    return img[cropy:y-cropy,cropx:x-cropx]    \n",
    "\n",
    "def imshow(img, size):\n",
    "  plt.cla()\n",
    "  plt.imshow(np.reshape(img, (size, size)), cmap='gray_r', vmin=0,vmax=1)\n",
    "  plt.pause(0.001)\n",
    "  plt.savefig(\"image.png\")\n",
    "\n",
    "def read_data(fname_root):\n",
    "  fname_img = fname_root + \"-images-idx3-ubyte\"\n",
    "  fname_lbl = fname_root + \"-labels-idx1-ubyte\"\n",
    "  with open(fname_lbl, 'rb') as flbl:\n",
    "    magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "  with open(fname_img, 'rb') as fimg:\n",
    "    magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "  return img.astype(np.float32), lbl.astype(int)\n",
    "\n",
    "def preprocess(x): return x/255\n",
    "\n",
    "def save_image(x_save, y_save, size, location, x_prefix, data_description):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)        \n",
    "    with open(location+'/'+x_prefix+'_desc.txt', 'w') as outfile:\n",
    "        outfile.write(data_description);        \n",
    "    with open(location+'/'+x_prefix+'_x_dump.json', 'w') as outfile:\n",
    "        json.dump(x_save.tolist(), outfile);\n",
    "    with open(location+'/'+x_prefix+'_y_dump.json', 'w') as outfile:\n",
    "        json.dump(y_save.tolist(), outfile);\n",
    "    plt.ioff()\n",
    "    \n",
    "    for i in range(np.size(x_save,0)):\n",
    "        img = np.reshape(x_save[i,:],(size,size))\n",
    "        plt.imshow(img, cmap='gray_r');\n",
    "        plt.savefig(location+'/'+x_prefix+'_'+str(i)+'.png', bbox_inches='tight')\n",
    "        plt.cla();\n",
    "    plt.ion()\n",
    "    \n",
    "    \n",
    "x_all, y_all = read_data(\"../datasets/raw/train\")\n",
    "x_test_all, y_test_all = read_data(\"../datasets/raw/t10k\")\n",
    "x_all, x_test_all = map(preprocess, [x_all, x_test_all])\n",
    "n_classes = int(1 + np.max(y_all))\n",
    "print(\"Data loaded\")\n",
    "\n",
    "#triplets = np.array([[0,1,5],[2,8,9],[3,6,7],[3,4,5]])\n",
    "triplet = [0,1,5]\n",
    "\n",
    "#for triplet in triplets:\n",
    "print(triplet)\n",
    "inds = (y_all==triplet[0]) + (y_all==triplet[1]) + (y_all==triplet[2]);\n",
    "inds_test = (y_test_all == triplet[0]) + (y_test_all == triplet[1]) + (y_test_all == triplet[2]);\n",
    "\n",
    "x,y = x_all[inds,:], y_all[inds];\n",
    "x_test,y_test = x_test_all[inds_test,:], y_test_all[inds_test];\n",
    "\n",
    "batch_size = 100\n",
    "num_train = int(batch_size*np.floor(x.shape[0]/batch_size))\n",
    "num_test = int(batch_size*np.floor(x_test.shape[0]/batch_size))\n",
    "\n",
    "x,y = x[0:num_train,:], y[0:num_train]\n",
    "x_test,y_test = x_test[0:num_test,:], y_test[0:num_test]\n",
    "\n",
    "y[y==triplet[0]]=10;\n",
    "y[y==triplet[1]]=11;\n",
    "y[y==triplet[2]]=12;\n",
    "\n",
    "y[y==10] = 0;\n",
    "y[y==11] = 1;\n",
    "y[y==12] = 2;\n",
    "\n",
    "y_test[y_test==triplet[0]]=10;\n",
    "y_test[y_test==triplet[1]]=11;\n",
    "y_test[y_test==triplet[2]]=12;\n",
    "\n",
    "y_test[y_test==10] = 0;\n",
    "y_test[y_test==11] = 1;\n",
    "y_test[y_test==12] = 2;\n",
    "\n",
    "y_adv_target = (y+np.random.randint(1,3,np.shape(y)))%3\n",
    "y_test_adv_target = (y_test+np.random.randint(1,3,np.shape(y_test)))%3\n",
    "\n",
    "print(x.shape, y.shape, x_test.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim1 = 15\n",
    "isize = dim1**2\n",
    "\n",
    "x_cropped = np.zeros((np.size(x,0),isize));\n",
    "x_smooth = np.zeros((np.size(x,0),isize));\n",
    "cur_perm = np.random.permutation(9)\n",
    "for i in range(np.size(x,0)):\n",
    "    img1 = crop_center(x[i,:].reshape(28,28), 3, 3)\n",
    "    #img1_scaled = img1\n",
    "    img1_scaled = rescale(img1, 15/22)\n",
    "    img2_scaled = np.clip(2*gaussian_filter(img1_scaled,0.5),0,1);\n",
    "    x_cropped[i,:] = img1_scaled.reshape(isize)\n",
    "    x_smooth[i,:] = img2_scaled.reshape(isize)\n",
    "\n",
    "x_cropped_test = np.zeros((np.size(x_test,0),isize));\n",
    "x_smooth_test = np.zeros((np.size(x_test,0),isize));\n",
    "for i in range(np.size(x_test,0)):\n",
    "    img1 = crop_center(x_test[i,:].reshape(28,28), 3, 3)\n",
    "    #img1_scaled = img1\n",
    "    img1_scaled = rescale(img1, 15/22)\n",
    "    img2_scaled = np.clip(2*gaussian_filter(img1_scaled,0.5),0,1);\n",
    "    x_cropped_test[i,:] = img1_scaled.reshape(isize)\n",
    "    x_smooth_test[i,:] = img2_scaled.reshape(isize)\n",
    "    \n",
    "\n",
    "\n",
    "zero_threshold = 0.1\n",
    "x_smooth[x_smooth<=zero_threshold]=zero_threshold\n",
    "x_smooth_test[x_smooth_test<=zero_threshold]=zero_threshold\n",
    "\n",
    "\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed);\n",
    "A = np.random.permutation(np.eye(isize));\n",
    "x_rot = np.dot(x_smooth,A);\n",
    "x_rot_test = np.dot(x_smooth_test,A);\n",
    "x_rot_test_adv = np.copy(x_rot_test);\n",
    "\n",
    "print(\"Data processing done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 225)\n"
     ]
    }
   ],
   "source": [
    "print(x_rot_test_adv[2900:3000,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model setup done\n"
     ]
    }
   ],
   "source": [
    "class PermutedMnistDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           train (boolean): Whether to instantiate with train or test dataset\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        if self.train==True:\n",
    "            self.images = torch.from_numpy(np.reshape(x_rot,[-1,1,dim1,dim1])).float()\n",
    "            #self.images = torch.from_numpy(np.reshape(x_smooth,[-1,1,dim1,dim1])).float()\n",
    "            self.labels = torch.from_numpy(y)\n",
    "            self.target_labels = torch.from_numpy(y_adv_target)\n",
    "        else:\n",
    "            self.images = torch.from_numpy(np.reshape(x_rot_test,[-1,1,dim1,dim1])).float()\n",
    "            #self.images = torch.from_numpy(np.reshape(x_smooth_test,[-1,1,dim1,dim1])).float()\n",
    "            self.labels = torch.from_numpy(y_test)\n",
    "            self.target_labels = torch.from_numpy(y_test_adv_target)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.images[idx], self.labels[idx],self.target_labels[idx])\n",
    "    \n",
    "train_dataset = PermutedMnistDataset(train=True)\n",
    "test_dataset = PermutedMnistDataset(train=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "num_classes = 3\n",
    "#batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        #self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(5 * 5 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "print(\"Model setup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0490, Accuracy: 99.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0114, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0026, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0149, Accuracy: 99.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0050, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0162, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0093, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0209, Accuracy: 99.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0005, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0004, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0643, Accuracy: 99.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0050, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0017, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.6 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels, _) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 90 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "            \n",
    "print(\"Starting testing...\")\n",
    "            \n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels,_ in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting adversarial training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.6763, Accuracy: 75.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.4264, Accuracy: 78.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.5101, Accuracy: 74.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.4739, Accuracy: 79.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.3866, Accuracy: 85.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.5084, Accuracy: 79.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.3687, Accuracy: 87.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.2064, Accuracy: 90.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.2308, Accuracy: 95.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.3326, Accuracy: 90.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.3741, Accuracy: 88.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.2678, Accuracy: 88.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.3221, Accuracy: 88.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.2368, Accuracy: 91.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.3204, Accuracy: 87.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.3012, Accuracy: 88.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.2429, Accuracy: 91.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.1608, Accuracy: 92.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.1607, Accuracy: 95.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.2859, Accuracy: 90.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 97.89999999999999 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting adversarial training...\")\n",
    "\n",
    "rob_thold = 0.2\n",
    "lrate_adv = 4e-2\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels,_) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images_original = images.clone()\n",
    "\n",
    "        for j in range(10):\n",
    "            images.requires_grad = True\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                images_grad_step = lrate_adv*torch.sign(images.grad)\n",
    "\n",
    "                #Take grad step\n",
    "                images += images_grad_step\n",
    "\n",
    "                #Project\n",
    "                images = images_original + torch.clamp(images-images_original, -rob_thold, rob_thold)\n",
    "                images = torch.clamp(images,0.1,1)\n",
    "        \n",
    "        images.requires_grad = False\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 90 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "print(\"Starting testing...\")\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels,_ in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting1\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "Test Accuracy of the model on the test images: 15.333333333333332 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting adversarial testing\")\n",
    "\n",
    "# Test the model on adversrial examples\n",
    "from torch.autograd import Variable\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "rob_thold = 0.2\n",
    "lrate_adv = 4e-2\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "count = 0\n",
    "for images, labels,target_labels in test_loader:\n",
    "    print(count)        \n",
    "    count+=batch_size\n",
    "    images, labels,target_labels = images.to(device), labels.to(device), target_labels.to(device)\n",
    "    images_original = images.clone()\n",
    "    \n",
    "    for i in range(50):\n",
    "        #print(i)\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #loss.backward()\n",
    "        inds_output = np.arange(batch_size)\n",
    "        outputs_sum = torch.sum(outputs[inds_output,target_labels])\n",
    "        outputs_sum.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            ##l-inf attack\n",
    "            images_grad_step = lrate_adv*torch.sign(images.grad)\n",
    "            \n",
    "            #Take grad step\n",
    "            images += images_grad_step\n",
    "            \n",
    "            #Project\n",
    "            images = images_original + torch.clamp(images-images_original, -rob_thold, rob_thold)\n",
    "            images = torch.clamp(images,0.1,1)\n",
    "            ##l-inf attack ends\n",
    "        \n",
    "\n",
    "            \n",
    "    outputs = model(images)\n",
    "    #print(count, count+batch_size,np.shape(x_rot_test_adv[count:count+batch_size,:]))\n",
    "    x_rot_test_adv[count-batch_size:count,:] = np.reshape(images.cpu().numpy(),(batch_size,dim1*dim1))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the test images: {} %'.format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_smooth_test_noisy = x_smooth_test+np.random.randint(0,2,x_smooth_test.shape)*rob_thold*2 - rob_thold\n",
    "x_smooth_test_noisy[x_smooth_test_noisy<=zero_threshold]=zero_threshold\n",
    "x_rot_test_noisy = np.dot(x_smooth_test_noisy,A)\n",
    "x_smooth_test_adv = np.dot(x_rot_test_adv, A.T)\n",
    "\n",
    "save_size = 1000\n",
    "\n",
    "np.savez('gen_data/temp',x_smooth[0:save_size,:],x_smooth_test[0:save_size,:],x_smooth_test_noisy[0:save_size,:],x_smooth_test_adv[0:save_size,:],x_rot[0:save_size,:],x_rot_test[0:save_size,:],x_rot_test_noisy[0:save_size,:],x_rot_test_adv[0:save_size,:],y[0:save_size],y_test[0:save_size],y_test_adv_target[0:save_size],random_seed)\n",
    "\n",
    "#tt=np.load('gen_data/temp.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7ac91bd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc82daac630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(tt['arr_2'][4,:],15)\n",
    "print(tt['arr_9'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "#imshow(x_rot_test_adv[2,:].dot(A.T),15)\n",
    "#plt.imshow(images[-1,0,:,:].cpu().numpy().reshape(isize).dot(A.T).reshape((15,15)), vmin=0, vmax=1, cmap='gray_r')\n",
    "#plt.imshow(images[-1,0,:,:].cpu().numpy(), vmin=0, vmax=1, cmap='gray_r')\n",
    "#print(y_test[2],y_test_adv_target[2])\n",
    "#print(outputs[1:10])\n",
    "\n",
    "print(np.min(x_rot_test_adv))\n",
    "\n",
    "#print(outputs[[1,2,3],labels[1:3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4c69505c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4c6868fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(x_rot_test[4,:],15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training svm model...')        \n",
    "\n",
    "#Choice of C makes a big difference sometimes\n",
    "model = svm.LinearSVC(max_iter = 5e3,C=0.001)\n",
    "\n",
    "model.fit(x_smooth,y)\n",
    "y_test_pred = model.predict(x_smooth_test)\n",
    "y_train_pred = model.predict(x_smooth);\n",
    "print('Train error = ', np.mean(y_train_pred!=y))\n",
    "print('Test error = ', np.mean(y_test_pred!=y_test))\n",
    "\n",
    "coefs = model.coef_;\n",
    "eps = 0.2;\n",
    "train_set = x_smooth[0:1000,:];\n",
    "test_set = x_smooth_test[0:1000,:];\n",
    "y_train_set = y[0:1000];\n",
    "y_test_set = y_test[0:1000];\n",
    "\n",
    "rot_train_set = x_rot[0:1000,:];\n",
    "rot_test_set = x_rot_test[0:1000,:];\n",
    "\n",
    "normal_noise = np.random.randint(0,2,np.shape(rot_test_set))*eps*2 - eps;\n",
    "test_set_noisy = np.clip(test_set + normal_noise,0,1);\n",
    "rot_test_set_noisy = np.dot(test_set_noisy ,A);\n",
    "\n",
    "y_adv_target = np.zeros(1000);\n",
    "test_set_adv = np.zeros(np.shape(test_set));\n",
    "\n",
    "for i in range(1000):\n",
    "    label = y_test_set[i];\n",
    "    if label==0:\n",
    "        target_label = np.random.randint(1,3)                \n",
    "    elif label == 1:\n",
    "        target_label = np.random.randint(0,2)*2;\n",
    "    elif label==2:\n",
    "        target_label = np.random.randint(0,2);\n",
    "    else:\n",
    "        print(\"error! undefined label.\")\n",
    "    #print('before:',human_perf[co,3])\n",
    "    y_adv_target[i] = target_label;\n",
    "\n",
    "    test_set_adv[i,:] = np.clip(test_set[i,:] + np.sign(coefs[target_label,:])*eps,0,1);\n",
    "\n",
    "rot_test_set_adv = np.dot(test_set_adv ,A);\n",
    "\n",
    "parent_dir = 'images_smooth/labels_'+str(triplet[0])+str(triplet[1])+str(triplet[2])+'/perm'+str(random_seed);\n",
    "\n",
    "\n",
    "save_image(train_set, y_train_set, 15, parent_dir+'/normal/train', 'train', 'Training data, normal images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(test_set, y_test_set, 15, parent_dir+'/normal/test', 'test', 'Test data, normal images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(test_set_noisy, y_test_set, 15, parent_dir+'/normal/test_noisy', 'test_noisy', 'Test data with +-0.2 noise, normal images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(test_set_adv, y_adv_target, 15, parent_dir+'/normal/test_adv', 'test_adv', 'Test data with linf 0.2 adversarial noise, normal images, 15 X 15, y dump contains target y label');\n",
    "print('Done');\n",
    "\n",
    "save_image(rot_train_set, y_train_set, 15, parent_dir+'/rotated/train', 'train', 'Training data, rotated images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(rot_test_set, y_test_set, 15, parent_dir+'/rotated/test', 'test', 'Test data, rotated images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(rot_test_set_noisy, y_test_set, 15, parent_dir+'/rotated/test_noisy', 'test_noisy', 'Test data with +-0.2 noise, rotated images, 15 X 15');\n",
    "print('Done');\n",
    "save_image(rot_test_set_adv, y_adv_target, 15, parent_dir+'/rotated/test_adv', 'test_adv', 'Test data with linf 0.2 adversarial noise, rotated images, 15 X 15, y dump contains target y label');\n",
    "print('Done');\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
