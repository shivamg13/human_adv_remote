{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "[0, 1, 5]\n",
      "(18000, 784) (18000,) (3000, 784) (3000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing done\n",
      "Model setup done\n",
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0653, Accuracy: 99.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0491, Accuracy: 98.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0036, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0265, Accuracy: 98.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0187, Accuracy: 99.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0015, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0072, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0011, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0056, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0008, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0004, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0026, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0089, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0017, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.2 %\n",
      "Starting adversarial testing\n",
      "Test Accuracy of the model on the test images: 21.933333333333334 %\n",
      "5 Done\n",
      "Data processing done\n",
      "Model setup done\n",
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0187, Accuracy: 100.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0338, Accuracy: 99.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0046, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0025, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0031, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0164, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0033, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0010, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0005, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0012, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0013, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0015, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0038, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.6 %\n",
      "Starting adversarial testing\n",
      "Test Accuracy of the model on the test images: 11.1 %\n",
      "6 Done\n",
      "Data processing done\n",
      "Model setup done\n",
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0619, Accuracy: 97.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0220, Accuracy: 99.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0114, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0115, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0117, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0033, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0004, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0050, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0002, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.56666666666666 %\n",
      "Starting adversarial testing\n",
      "Test Accuracy of the model on the test images: 12.0 %\n",
      "7 Done\n",
      "Data processing done\n",
      "Model setup done\n",
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0634, Accuracy: 97.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0167, Accuracy: 99.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0232, Accuracy: 99.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0086, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0067, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0120, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0017, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0060, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0435, Accuracy: 99.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0010, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0236, Accuracy: 99.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0045, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0004, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.56666666666666 %\n",
      "Starting adversarial testing\n",
      "Test Accuracy of the model on the test images: 6.966666666666667 %\n",
      "8 Done\n",
      "Data processing done\n",
      "Model setup done\n",
      "Starting training...\n",
      "Epoch [1/10], Step [90/180], Loss: 0.0552, Accuracy: 99.00%\n",
      "Epoch [1/10], Step [180/180], Loss: 0.0106, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [90/180], Loss: 0.0077, Accuracy: 100.00%\n",
      "Epoch [2/10], Step [180/180], Loss: 0.0213, Accuracy: 99.00%\n",
      "Epoch [3/10], Step [90/180], Loss: 0.0058, Accuracy: 100.00%\n",
      "Epoch [3/10], Step [180/180], Loss: 0.0005, Accuracy: 100.00%\n",
      "Epoch [4/10], Step [90/180], Loss: 0.0094, Accuracy: 99.00%\n",
      "Epoch [4/10], Step [180/180], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [90/180], Loss: 0.0010, Accuracy: 100.00%\n",
      "Epoch [5/10], Step [180/180], Loss: 0.0037, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [90/180], Loss: 0.0037, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [180/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [90/180], Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [7/10], Step [180/180], Loss: 0.0360, Accuracy: 99.00%\n",
      "Epoch [8/10], Step [90/180], Loss: 0.0070, Accuracy: 100.00%\n",
      "Epoch [8/10], Step [180/180], Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [90/180], Loss: 0.0016, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [180/180], Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [90/180], Loss: 0.0012, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [180/180], Loss: 0.0009, Accuracy: 100.00%\n",
      "Starting testing...\n",
      "Test Accuracy of the model on the 10000 test images: 99.66666666666667 %\n",
      "Starting adversarial testing\n",
      "Test Accuracy of the model on the test images: 14.7 %\n",
      "9 Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "import os\n",
    "from scipy import stats\n",
    "import struct\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.linalg import eigh\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy as sp\n",
    "import math\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.transform import resize\n",
    "from skimage.transform import rescale\n",
    "import png\n",
    "import imageio\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "def crop_center(img,cropy, cropx):\n",
    "    y,x = img.shape\n",
    "    return img[cropy:y-cropy,cropx:x-cropx]    \n",
    "\n",
    "def imshow(img, size):\n",
    "  plt.cla()\n",
    "  plt.imshow(np.reshape(img, (size, size)), cmap='gray_r', vmin=0,vmax=1)\n",
    "  plt.pause(0.001)\n",
    "  plt.savefig(\"image.png\")\n",
    "\n",
    "def read_data(fname_root):\n",
    "  fname_img = fname_root + \"-images-idx3-ubyte\"\n",
    "  fname_lbl = fname_root + \"-labels-idx1-ubyte\"\n",
    "  with open(fname_lbl, 'rb') as flbl:\n",
    "    magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "  with open(fname_img, 'rb') as fimg:\n",
    "    magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "  return img.astype(np.float32), lbl.astype(int)\n",
    "\n",
    "def preprocess(x): return x/255\n",
    "\n",
    "def save_image(x_save, y_save, size, location, x_prefix, data_description):\n",
    "    if not os.path.exists(location):\n",
    "        os.makedirs(location)        \n",
    "    with open(location+'/'+x_prefix+'_desc.txt', 'w') as outfile:\n",
    "        outfile.write(data_description);        \n",
    "    with open(location+'/'+x_prefix+'_x_dump.json', 'w') as outfile:\n",
    "        json.dump(x_save.tolist(), outfile);\n",
    "    with open(location+'/'+x_prefix+'_y_dump.json', 'w') as outfile:\n",
    "        json.dump(y_save.tolist(), outfile);\n",
    "    plt.ioff()\n",
    "    \n",
    "    for i in range(np.size(x_save,0)):\n",
    "        img = np.reshape(x_save[i,:],(size,size))\n",
    "        plt.imshow(img, cmap='gray_r');\n",
    "        plt.savefig(location+'/'+x_prefix+'_'+str(i)+'.png', bbox_inches='tight')\n",
    "        plt.cla();\n",
    "    plt.ion()\n",
    "    \n",
    "    \n",
    "x_all, y_all = read_data(\"../datasets/raw/train\")\n",
    "x_test_all, y_test_all = read_data(\"../datasets/raw/t10k\")\n",
    "x_all, x_test_all = map(preprocess, [x_all, x_test_all])\n",
    "n_classes = int(1 + np.max(y_all))\n",
    "print(\"Data loaded\")\n",
    "\n",
    "#triplets = np.array([[0,1,5],[2,8,9],[3,6,7],[3,4,5]])\n",
    "triplet = [0,1,5]\n",
    "\n",
    "#for triplet in triplets:\n",
    "print(triplet)\n",
    "inds = (y_all==triplet[0]) + (y_all==triplet[1]) + (y_all==triplet[2]);\n",
    "inds_test = (y_test_all == triplet[0]) + (y_test_all == triplet[1]) + (y_test_all == triplet[2]);\n",
    "\n",
    "x,y = x_all[inds,:], y_all[inds];\n",
    "x_test,y_test = x_test_all[inds_test,:], y_test_all[inds_test];\n",
    "\n",
    "batch_size = 100\n",
    "num_train = int(batch_size*np.floor(x.shape[0]/batch_size))\n",
    "num_test = int(batch_size*np.floor(x_test.shape[0]/batch_size))\n",
    "\n",
    "x,y = x[0:num_train,:], y[0:num_train]\n",
    "x_test,y_test = x_test[0:num_test,:], y_test[0:num_test]\n",
    "\n",
    "y[y==triplet[0]]=10;\n",
    "y[y==triplet[1]]=11;\n",
    "y[y==triplet[2]]=12;\n",
    "\n",
    "y[y==10] = 0;\n",
    "y[y==11] = 1;\n",
    "y[y==12] = 2;\n",
    "\n",
    "y_test[y_test==triplet[0]]=10;\n",
    "y_test[y_test==triplet[1]]=11;\n",
    "y_test[y_test==triplet[2]]=12;\n",
    "\n",
    "y_test[y_test==10] = 0;\n",
    "y_test[y_test==11] = 1;\n",
    "y_test[y_test==12] = 2;\n",
    "\n",
    "\n",
    "print(x.shape, y.shape, x_test.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim1 = 15\n",
    "isize = dim1**2\n",
    "\n",
    "x_cropped = np.zeros((np.size(x,0),isize));\n",
    "x_smooth = np.zeros((np.size(x,0),isize));\n",
    "cur_perm = np.random.permutation(9)\n",
    "for i in range(np.size(x,0)):\n",
    "    img1 = crop_center(x[i,:].reshape(28,28), 3, 3)\n",
    "    #img1_scaled = img1\n",
    "    img1_scaled = rescale(img1, 15/22)\n",
    "    img2_scaled = np.clip(2*gaussian_filter(img1_scaled,0.5),0,1);\n",
    "    x_cropped[i,:] = img1_scaled.reshape(isize)\n",
    "    x_smooth[i,:] = img2_scaled.reshape(isize)\n",
    "\n",
    "x_cropped_test = np.zeros((np.size(x_test,0),isize));\n",
    "x_smooth_test = np.zeros((np.size(x_test,0),isize));\n",
    "for i in range(np.size(x_test,0)):\n",
    "    img1 = crop_center(x_test[i,:].reshape(28,28), 3, 3)\n",
    "    #img1_scaled = img1\n",
    "    img1_scaled = rescale(img1, 15/22)\n",
    "    img2_scaled = np.clip(2*gaussian_filter(img1_scaled,0.5),0,1);\n",
    "    x_cropped_test[i,:] = img1_scaled.reshape(isize)\n",
    "    x_smooth_test[i,:] = img2_scaled.reshape(isize)\n",
    "    \n",
    "\n",
    "\n",
    "zero_threshold = 0.1\n",
    "x_smooth[x_smooth<=zero_threshold]=zero_threshold\n",
    "x_smooth_test[x_smooth_test<=zero_threshold]=zero_threshold\n",
    "\n",
    "\n",
    "for random_seed in np.arange(5,10):\n",
    "    np.random.seed(random_seed);\n",
    "    A = np.random.permutation(np.eye(isize));\n",
    "    x_rot = np.dot(x_smooth,A);\n",
    "    x_rot_test = np.dot(x_smooth_test,A);\n",
    "    x_rot_test_adv = np.copy(x_rot_test);\n",
    "\n",
    "    y_adv_target = (y+np.random.randint(1,3,np.shape(y)))%3\n",
    "    y_test_adv_target = (y_test+np.random.randint(1,3,np.shape(y_test)))%3\n",
    "\n",
    "    print(\"Data processing done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class PermutedMnistDataset(Dataset):\n",
    "        \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "        def __init__(self, train):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "               train (boolean): Whether to instantiate with train or test dataset\n",
    "            \"\"\"\n",
    "            self.train = train\n",
    "            if self.train==True:\n",
    "                self.images = torch.from_numpy(np.reshape(x_rot,[-1,1,dim1,dim1])).float()\n",
    "                #self.images = torch.from_numpy(np.reshape(x_smooth,[-1,1,dim1,dim1])).float()\n",
    "                self.labels = torch.from_numpy(y)\n",
    "                self.target_labels = torch.from_numpy(y_adv_target)\n",
    "            else:\n",
    "                self.images = torch.from_numpy(np.reshape(x_rot_test,[-1,1,dim1,dim1])).float()\n",
    "                #self.images = torch.from_numpy(np.reshape(x_smooth_test,[-1,1,dim1,dim1])).float()\n",
    "                self.labels = torch.from_numpy(y_test)\n",
    "                self.target_labels = torch.from_numpy(y_test_adv_target)\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.images[idx], self.labels[idx],self.target_labels[idx])\n",
    "\n",
    "    train_dataset = PermutedMnistDataset(train=True)\n",
    "    test_dataset = PermutedMnistDataset(train=False)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epochs = 10\n",
    "    num_classes = 3\n",
    "    #batch_size = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "\n",
    "    ###############################\n",
    "\n",
    "    # Data loader\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Convolutional neural network (two convolutional layers)\n",
    "    class ConvNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=4, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            #self.drop_out = nn.Dropout()\n",
    "            self.fc1 = nn.Linear(5 * 5 * 64, 1000)\n",
    "            self.fc2 = nn.Linear(1000, 3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.layer1(x)\n",
    "            out = self.layer2(out)\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            #out = self.drop_out(out)\n",
    "            out = self.fc1(out)\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "    model = ConvNet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    print(\"Model setup done\")\n",
    "\n",
    "\n",
    "    #############################\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels, _) in enumerate(train_loader):\n",
    "            # Run the forward pass\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and perform Adam optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "\n",
    "            if (i + 1) % 90 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                              (correct / total) * 100))\n",
    "\n",
    "\n",
    "    print(\"Starting testing...\")\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels,_ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    print(\"Starting adversarial testing\")\n",
    "\n",
    "    # Test the model on adversrial examples\n",
    "    from torch.autograd import Variable\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "    rob_thold = 0.2\n",
    "    lrate_adv = 4e-2\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for images, labels,target_labels in test_loader:\n",
    "        #print(count)        \n",
    "        count+=batch_size\n",
    "        images, labels,target_labels = images.to(device), labels.to(device), target_labels.to(device)\n",
    "        images_original = images.clone()\n",
    "\n",
    "        for i in range(50):\n",
    "            #print(i)\n",
    "            images.requires_grad = True\n",
    "            outputs = model(images)\n",
    "            #loss = criterion(outputs, labels)\n",
    "            #loss.backward()\n",
    "            inds_output = np.arange(batch_size)\n",
    "            outputs_sum = torch.sum(outputs[inds_output,target_labels])\n",
    "            outputs_sum.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                ##l-inf attack\n",
    "                images_grad_step = lrate_adv*torch.sign(images.grad)\n",
    "\n",
    "                #Take grad step\n",
    "                images += images_grad_step\n",
    "\n",
    "                #Project\n",
    "                images = images_original + torch.clamp(images-images_original, -rob_thold, rob_thold)\n",
    "                images = torch.clamp(images,0.1,1)\n",
    "                ##l-inf attack ends\n",
    "\n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "        #print(count, count+batch_size,np.shape(x_rot_test_adv[count:count+batch_size,:]))\n",
    "        x_rot_test_adv[count-batch_size:count,:] = np.reshape(images.cpu().numpy(),(batch_size,dim1*dim1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "\n",
    "    ############################\n",
    "\n",
    "    x_smooth_test_noisy = x_smooth_test+np.random.randint(0,2,x_smooth_test.shape)*rob_thold*2 - rob_thold\n",
    "    x_smooth_test_noisy[x_smooth_test_noisy<=zero_threshold]=zero_threshold\n",
    "    x_rot_test_noisy = np.dot(x_smooth_test_noisy,A)\n",
    "    x_smooth_test_adv = np.dot(x_rot_test_adv, A.T)\n",
    "\n",
    "    save_size = 1000\n",
    "\n",
    "    np.savez('gen_data/data'+str(random_seed),x_smooth[0:save_size,:],x_smooth_test[0:save_size,:],x_smooth_test_noisy[0:save_size,:],x_smooth_test_adv[0:save_size,:],x_rot[0:save_size,:],x_rot_test[0:save_size,:],x_rot_test_noisy[0:save_size,:],x_rot_test_adv[0:save_size,:],y[0:save_size],y_test[0:save_size],y_test_adv_target[0:save_size],random_seed)\n",
    "\n",
    "    #tt=np.load('gen_data/temp.npz')\n",
    "    print(str(random_seed)+\" Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
